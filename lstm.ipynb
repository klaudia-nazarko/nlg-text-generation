{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQFscujsh+LyLPYNFCPS1Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/klaudia-nazarko/nlg-text-generation/blob/main/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "535ilqDV9kjd"
      },
      "source": [
        "# Word-Level Text Generation with LSTM\r\n",
        "\r\n",
        "In addition to making predictions, RNNs may also be used as generative models (can learn the sequences and then generate entirely new seqences). One of RNN variant, LSTM neural network has been recognized as a very successful tool when working with sequences of letters or words.\r\n",
        "\r\n",
        "Let's examine performance of basic LSTM model on generating text of fairy tales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFchVFdsCBEw",
        "outputId": "0edb508d-37f1-4351-8970-aca7d3ecd1c9"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive', force_remount=True)\r\n",
        "%cd 'drive/MyDrive/Colab Notebooks/nlg_tales_generation'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/nlg_tales_generation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbE3BBh68v55"
      },
      "source": [
        "import functions as f\r\n",
        "\r\n",
        "from Text import *\r\n",
        "from LSTM_class import *\r\n",
        "\r\n",
        "from keras import layers, models, optimizers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUqK-M-p1raQ"
      },
      "source": [
        "## Text preprocessing\r\n",
        "\r\n",
        "The loaded text file contains the content of tales scraped from websites. By creating the instance of Text object, the text is quickly preprocessed and tokenized; by creating the instance of Sequence object the text is prepared for use in LSTM model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLHty8SmmueL"
      },
      "source": [
        "path_train, path_test = 'data/train.txt', 'data/test.txt'\r\n",
        "\r\n",
        "input_train = f.read_txt(path_train)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZQU63Rz10_C",
        "outputId": "4d36716a-bb41-4fbb-9faa-4dc09243409b"
      },
      "source": [
        "max_len = 4\r\n",
        "step = 3\r\n",
        "\r\n",
        "text_train = Text(input_train)\r\n",
        "text_train.tokens_info()\r\n",
        "\r\n",
        "seq_train = Sequences(text_train, max_len, step)\r\n",
        "seq_train.sequences_info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total tokens: 890750, distinct tokens: 25165\n",
            "number of sequences of length 4: 296916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXG4cBuABXM2"
      },
      "source": [
        "The text is split into sequences of length 4 (max_len parameter) with step 3. We can see that the first sequence of 4 words starts with the first (0-index) word and the second sequence starts after 3 words, so from the 4th word (3-index)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er-A4Yv2DnoD",
        "outputId": "a016fa39-1885-4a6a-bb61-f17f734e896d"
      },
      "source": [
        "print(text_train.tokens[:10])\r\n",
        "print(text_train.tokens_ind[:10], '\\n')\r\n",
        "\r\n",
        "np.array(seq_train.sequences[:2])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Once', 'upon', 'a', 'time', 'there', 'lived', 'a', 'sultan', 'who', 'loved']\n",
            "[10701, 17952, 19552, 289, 10967, 9397, 19552, 21301, 6393, 1702] \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10701, 17952, 19552,   289],\n",
              "       [  289, 10967,  9397, 19552]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGR6r0WpBeC8"
      },
      "source": [
        "TextDataGenerator is a Python generator that outputs batches of data (sequences and corresponding next words). Since the vocabulary size is over 25k, it's impossible to fit all data to the memory and that's why batch generator is extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Hw3av9-v1TO"
      },
      "source": [
        "batch_size = 512\r\n",
        "\r\n",
        "params = {\r\n",
        "  'sequence_length': max_len,\r\n",
        "  'vocab_size': len(text_train),\r\n",
        "  'batch_size': batch_size,\r\n",
        "  'shuffle': True\r\n",
        "}\r\n",
        "\r\n",
        "train_generator = TextDataGenerator(seq_train.sequences, seq_train.next_words, **params)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG0GSvi8_anC"
      },
      "source": [
        "## Training the LSTM model\r\n",
        "\r\n",
        "We'll build a simple model with one LSTM layer, dropout and dense layer with softmax activation (to return word probabilities)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAzlW7zXSWcK"
      },
      "source": [
        "def lstm_model(sequence_length, vocab_size, layer_size, embedding=False):\r\n",
        "  model = models.Sequential()\r\n",
        "  if embedding:\r\n",
        "    model.add(layers.Embedding(vocab_size, layer_size))\r\n",
        "    model.add(layers.LSTM(layer_size))    \r\n",
        "  else:\r\n",
        "    model.add(layers.LSTM(layer_size, input_shape=(sequence_length, vocab_size)))\r\n",
        "  model.add(layers.Dropout(0.3))\r\n",
        "  model.add(layers.Dense(vocab_size, activation='softmax'))\r\n",
        "  return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZfL534BwLiO"
      },
      "source": [
        "model = lstm_model(max_len, len(text_train), 512)\r\n",
        "\r\n",
        "optimizer = optimizers.RMSprop(lr=0.01)\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfRwajqPwWPi",
        "outputId": "b0262441-b985-42b6-b074-892961a8ca62"
      },
      "source": [
        "model.fit(train_generator,\r\n",
        "          steps_per_epoch=len(train_generator),\r\n",
        "          epochs=20)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "579/579 [==============================] - 155s 254ms/step - loss: 6.4620\n",
            "Epoch 2/20\n",
            "579/579 [==============================] - 150s 258ms/step - loss: 5.1252\n",
            "Epoch 3/20\n",
            "579/579 [==============================] - 150s 259ms/step - loss: 4.8568\n",
            "Epoch 4/20\n",
            "579/579 [==============================] - 150s 258ms/step - loss: 4.5900\n",
            "Epoch 5/20\n",
            "579/579 [==============================] - 150s 259ms/step - loss: 4.2387\n",
            "Epoch 6/20\n",
            "579/579 [==============================] - 150s 258ms/step - loss: 3.9165\n",
            "Epoch 7/20\n",
            "579/579 [==============================] - 149s 258ms/step - loss: 3.6732\n",
            "Epoch 8/20\n",
            "579/579 [==============================] - 150s 259ms/step - loss: 3.4832\n",
            "Epoch 9/20\n",
            "579/579 [==============================] - 149s 258ms/step - loss: 3.3406\n",
            "Epoch 10/20\n",
            "579/579 [==============================] - 149s 258ms/step - loss: 3.1895\n",
            "Epoch 11/20\n",
            "579/579 [==============================] - 150s 258ms/step - loss: 3.0772\n",
            "Epoch 12/20\n",
            "579/579 [==============================] - 149s 258ms/step - loss: 2.9779\n",
            "Epoch 13/20\n",
            "579/579 [==============================] - 150s 259ms/step - loss: 2.8850\n",
            "Epoch 14/20\n",
            "579/579 [==============================] - 149s 258ms/step - loss: 2.7920\n",
            "Epoch 15/20\n",
            "579/579 [==============================] - 150s 259ms/step - loss: 2.7638\n",
            "Epoch 16/20\n",
            "579/579 [==============================] - 149s 258ms/step - loss: 2.7118\n",
            "Epoch 17/20\n",
            "579/579 [==============================] - 149s 258ms/step - loss: 2.6552\n",
            "Epoch 18/20\n",
            "579/579 [==============================] - 150s 259ms/step - loss: 2.6189\n",
            "Epoch 19/20\n",
            "579/579 [==============================] - 150s 259ms/step - loss: 2.5916\n",
            "Epoch 20/20\n",
            "579/579 [==============================] - 149s 258ms/step - loss: 2.5677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0964180dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpRxjstuTMVL",
        "outputId": "799f3e78-7796-4283-8a7f-fc8f6a22a489"
      },
      "source": [
        "model.save('data/out/lstm_model')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: data/out/lstm_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: data/out/lstm_model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKx_NBOV_iey"
      },
      "source": [
        "## Text generation with LSTM model\r\n",
        "\r\n",
        "Generating text with LSTM model requires building the prediction loop which starts with choosing a prefix and setting the number of words to generate. Then we need to predict the next word using our LSTM model and use this word as part of the prefix for the next model input. The loop is executed until the expected number of words is generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGU38wwtTN1J"
      },
      "source": [
        "#model = models.load_model('data/out/lstm_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwJRWQIvnp1I"
      },
      "source": [
        "token2ind, ind2token = text_train.token2ind, text_train.ind2token\r\n",
        "\r\n",
        "input_prefix = 'Once upon a time'\r\n",
        "text_prefix = Text(input_prefix, token2ind, ind2token)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B32EAv2mxsa1"
      },
      "source": [
        "pred = ModelPredict(model, text_prefix, token2ind, ind2token, max_len)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccvYg3JO2xI1",
        "outputId": "4ef98896-4526-4c5b-f3fb-c8a95ab964f0"
      },
      "source": [
        "temperatures = [1, 0.7, 0.4, 0.1]\r\n",
        "\r\n",
        "for temperature in temperatures:\r\n",
        "  print('temperature:', temperature)\r\n",
        "  print(pred.generate_sequence(100, temperature=0.7))\r\n",
        "  print('\\n')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "temperature: 1\n",
            "Once upon a time there was a son. This lived she couple of so stand up the horse, and his brother and it was not long before he fell to dost thou come, There is the one of the lion, saying : days is still in his great life. I went back to the old woman. The girl then said, Prince Ivan, and was not a only one where his mother and her own child, she put on the right, he would show you the way to you; and I return to it\n",
            "\n",
            "\n",
            "temperature: 0.7\n",
            "Once upon a time there was a child, and they made their children have the life in him. If you do what you only to do him, and what had because she had been passed by the cat, which had such a most beautiful woman, a front of them cried : This is all my daughter. And she got a letter, and the poor man was not long son of the king and queen and her mother, he came back, and his wife and the children and rage between them all. She had a\n",
            "\n",
            "\n",
            "temperature: 0.4\n",
            "Once upon a time there was a long time ago, and on they went, and they asked her mother her. She had to get the boys. well. If it fell on the back, the room of water to sleep. And the deep disappeared in a touch. The of your soon shall be happy. As for the rest. <| end of text |> The king of a once! a sell the one of the speak of him again, and he was the take a care of your name is beautiful girl?'\n",
            "\n",
            "\n",
            "temperature: 0.1\n",
            "Once upon a time there was a proud and at her face with his hands, and in a few minutes she had got up and true the little most secret chamber, When I heard the next day he went to perhaps he gave the better of him, for he knew that his put the let down his horse after he had got the first way that he had not happy. Oh, says he, village, father and where he had home. Then the king whispered into the I. And he says, she was the end\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xe6ha7SayvQg"
      },
      "source": [
        "## Text generation with LSTM model with Embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WiKwkcXyu6C"
      },
      "source": [
        "batch_size_emb = 256\r\n",
        "\r\n",
        "params_emb = params.copy()\r\n",
        "params_emb['embedding'] = True\r\n",
        "\r\n",
        "train_generator_emb = TextDataGenerator(seq_train.sequences, seq_train.next_words, **params_emb)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjeejnRMzEyq"
      },
      "source": [
        "model_emb = lstm_model(max_len, len(text_train), 256, embedding=True)\r\n",
        "model_emb.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68tjpQ9tzXo7",
        "outputId": "add8180c-3b4c-4f6b-ab4b-5297189eda1d"
      },
      "source": [
        "model_emb.fit(train_generator_emb,\r\n",
        "              steps_per_epoch=len(train_generator_emb),\r\n",
        "              epochs=2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1159/1159 [==============================] - 50s 42ms/step - loss: 6.0971\n",
            "Epoch 2/2\n",
            "1159/1159 [==============================] - 47s 41ms/step - loss: 5.5615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efe6e587c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_VbeU-vzeHm",
        "outputId": "4c022781-b01a-45f2-b08f-ac99bc45bd21"
      },
      "source": [
        "model_emb.save('data/out/lstm_model_emb')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: data/out/lstm_model_emb/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: data/out/lstm_model_emb/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWlpBP6lzhpk"
      },
      "source": [
        "#model_emb = models.load_model('data/out/lstm_model_emb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nea-1TkMzped"
      },
      "source": [
        "pred_emb = ModelPredict(model_emb, text_prefix, token2ind, ind2token, max_len, embedding=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWnwmx8VzxNB",
        "outputId": "628e70e0-e390-4f72-b7e5-74d713ecab6f"
      },
      "source": [
        "temperatures = [1, 0.7, 0.4, 0.1]\r\n",
        "\r\n",
        "for temperature in temperatures:\r\n",
        "  print('temperature:', temperature)\r\n",
        "  print(pred_emb.generate_sequence(100, temperature=0.7))\r\n",
        "  print('\\n')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "temperature: 1\n",
            "Once upon a time she was to, and fantastical his love for the, the old man, and' avenging he. you to a a yoked the the children, and the King stithy to a little privilege, and foods to a'God to. It Desert I that thou smoky it. And he, she was a the man, he they shall abdicated reindeer a were pleads. So the I was so he and the and attentive. And he the his time to the king, he she had Wife to thou hast a.\n",
            "\n",
            "\n",
            "temperature: 0.7\n",
            "Once upon a time the Increase the.' waggle he he told him to the little Christendom, until he he he fell to the. After the he from the basing the- a hashish it this they had to. One of the he was fodder the they had a this to the a. The the it in the defend it at the he we have a it.' rankled in to the Cut that they had lifestyle to of her Free to. The nane it, for the they. They were flits. And he the\n",
            "\n",
            "\n",
            "temperature: 0.4\n",
            "Once upon a time there was as if all anymore he, the [Virgil it out with the to, but the he were, the I. He was. The man was in the water. he cried, he was the king the I, and he he thought it was as he should'Abbas that the till he he the men that he, and Whither she, because he had, and loyalty to of the luxuriantly he said, the he But this he I he asked, and the door, he not; and he them\n",
            "\n",
            "\n",
            "temperature: 0.1\n",
            "Once upon a time was nestlings it, too obvious he. No communion it answered, he she to the to said, the in the king he to do. I for their dose to! And he they of that the he, as he she, and interpreted to me and learner to a good, and told the it he was maxims it it was grit, as he he not send a depression to!--.-- the will have it to be throth it, he he The man. he that good DÃ¦mon\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUtj7TdP2Npc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}